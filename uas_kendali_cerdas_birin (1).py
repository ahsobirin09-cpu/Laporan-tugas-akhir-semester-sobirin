# -*- coding: utf-8 -*-
"""UAS kendali cerdas Birin

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QANGeTc1koL-2REKUwluHk1cr38sTsLH

##UPLOAD API KAGGLE
"""

# Install library kaggle jika belum adanya
!pip install -q kaggle

# Upload file kaggle.json yang sudah diunduh tadi
from google.colab import files
files.upload()

# Membuat direktori kaggle dan mengatur izin akses
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Cek apakah koneksi berhasil (akan menampilkan daftar dataset populer)
!kaggle datasets list

"""##DOWNLOAD DATASET"""

# Mengunduh dataset menggunakan API Command dari Kaggle
!kaggle datasets download -d arashnic/book-recommendation-dataset

"""##EKSTRAK DATESET"""

# Mengekstrak file zip yang baru saja diunduh
!unzip book-recommendation-dataset.zip -d data_buku

"""##Data Understanding (Memahami Data)"""

import pandas as pd

# Load data
books = pd.read_csv('data_buku/Books.csv', low_memory=False)
ratings = pd.read_csv('data_buku/Ratings.csv')
users = pd.read_csv('data_buku/Users.csv')

# Cek jumlah data (Kriteria Wajib: Memberikan informasi jumlah data)
print(f"Jumlah judul buku: {len(books)}")
print(f"Jumlah data rating: {len(ratings)}")
print(f"Jumlah user: {len(users)}")

# Cek missing values (Kriteria Wajib: Kondisi data)
print("\nMissing values pada Books:\n", books.isnull().sum())

"""##Data Preparation (Persiapan Data)"""

# 1. Menggabungkan data Rating dengan Buku (Merging)
# Alasan: Agar kita tahu judul buku mana yang diberikan rating oleh user
book_info = pd.merge(ratings, books, on='ISBN')

# 2. Menghapus kolom yang tidak diperlukan (Drop columns)
# Alasan: Mengurangi beban memori dan menghapus fitur yang tidak relevan bagi model
unused_cols = ['Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'Year-Of-Publication', 'Publisher']
book_info = book_info.drop(unused_cols, axis=1)

# 3. Membersihkan data duplikat
# Alasan: Memastikan satu user hanya memberikan satu rating untuk satu buku
book_info = book_info.drop_duplicates(['User-ID', 'Book-Title'])

# 4. Filter data (Opsional tapi disarankan)
# Alasan: Mengurangi noise dengan hanya mengambil buku yang memiliki cukup banyak rating
top_books = book_info.groupby('Book-Title')['Book-Rating'].count().reset_index()
top_books = top_books[top_books['Book-Rating'] >= 50] # Contoh: minimal 50 rating

"""##Modeling (Content-Based Filtering)"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Mengambil sampel data agar tidak terlalu berat (opsional jika RAM terbatas)
# Kita ambil unik berdasarkan judul buku
data_content = book_info.drop_duplicates('Book-Title').head(10000)

# Inisialisasi TF-IDF
tf = TfidfVectorizer()
tfidf_matrix = tf.fit_transform(data_content['Book-Author'].astype(str))

# Menghitung Cosine Similarity
cosine_sim = cosine_similarity(tfidf_matrix)

"""##Result & Evaluation"""



def get_recommendations(title, cosine_sim=cosine_sim):
    idx = data_content[data_content['Book-Title'] == title].index[0]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Ambil 5 rekomendasi teratas (Top-5)
    book_indices = [i[0] for i in sim_scores[1:6]]
    return data_content['Book-Title'].iloc[book_indices]

# Test model
print(get_recommendations('Classical Mythology'))

